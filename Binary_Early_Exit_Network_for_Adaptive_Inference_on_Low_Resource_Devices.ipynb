{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xsKxMDEowGUy",
        "Q6oysm1Iz6VT",
        "AX2crGZmAHEA",
        "y9yWjwYqBCda"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices \n",
        "### Interspeech 2022 \n",
        "----------------------------------- \n",
        "## Abstract\n",
        "Deep neural networks have significantly improved performance\n",
        "on a range of tasks with the increasing demand for computational resources, leaving deployment on low-resource devices\n",
        "(with limited memory and battery power) infeasible. Binary\n",
        "neural networks (BNNs) tackle the issue to an extent with extreme compression and speed-up gains compared to real-valued\n",
        "models. We propose a simple but effective method to accelerate\n",
        "inference through unifying BNNs with an early-exiting strategy.\n",
        "Our approach allows simple instances to exit early based on a\n",
        "decision threshold and utilizes output layers added to different\n",
        "intermediate layers to avoid executing the entire binary model.\n",
        "We extensively evaluate our method on three audio classification tasks and across four BNNs architectures. Our method\n",
        "demonstrates favorable quality-efficiency trade-offs while being\n",
        "controllable with an entropy-based threshold specified by the\n",
        "system user. It also results in better speed-ups (latency less than\n",
        "6ms) with a single model based on existing BNN architectures\n",
        "without retraining for different efficiency levels. It also provides\n",
        "a straightforward way to estimate sample difficulty and better\n",
        "understanding of uncertainty around certain classes within the\n",
        "dataset.\n",
        "\n",
        "## Paper\n",
        "https://arxiv.org/pdf/2206.09029.pdf\n",
        "\n",
        "----------------------------------- \n",
        "\n",
        "**Toy example for training a tiny early-exit BNN model on SpeechCommands dataset.**"
      ],
      "metadata": {
        "id": "ERF0VNLqAPJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Utils"
      ],
      "metadata": {
        "id": "xsKxMDEowGUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install larq\n",
        "!pip install pydub\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import larq as lq\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "cBkB2mtw_lVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_spectrogram(waveform,\n",
        "    sample_rate = 16000, frame_length = 400, frame_step = 160, \n",
        "    fft_length = 1024, n_mels = 64, fmin = 60.0, fmax = 7800.0):\n",
        "\n",
        "    stfts = tf.signal.stft(waveform, \n",
        "        frame_length=frame_length, \n",
        "        frame_step=frame_step,\n",
        "        fft_length=fft_length)\n",
        "    spectrograms = tf.abs(stfts)\n",
        "\n",
        "    num_spectrogram_bins = stfts.shape[-1] \n",
        "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = fmin, fmax, n_mels\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
        "        upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(\n",
        "        spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
        "        linear_to_mel_weight_matrix.shape[-1:]))\n",
        "\n",
        "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "    return log_mel_spectrograms\n",
        "\n",
        "def prepare_example(waveform, label, sequence_length=16000):\n",
        "    waveform = tf.cast(waveform, tf.float32) / float(tf.int16.max)\n",
        "    padding = tf.maximum(sequence_length - tf.shape(waveform)[0], 0)\n",
        "    left_pad = padding // 2\n",
        "    right_pad = padding - left_pad\n",
        "    waveform = tf.pad(waveform, paddings=[[left_pad, right_pad]])\n",
        "    log_mel_spectrogram = generate_spectrogram(waveform)        \n",
        "    return log_mel_spectrogram[Ellipsis, tf.newaxis], label"
      ],
      "metadata": {
        "id": "-T4NW1H-0A0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Dataset (SpeechCommands)"
      ],
      "metadata": {
        "id": "Q6oysm1Iz6VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "test_batch_size = 1\n",
        "shuffle_buffer = 1024\n",
        "autotune = tf.data.AUTOTUNE\n",
        "\n",
        "(ds_train, ds_test), ds_info = tfds.load(\"speech_commands\", \n",
        "    split=[\"train\", \"test\"], shuffle_files=True, \n",
        "    as_supervised=True, with_info=True)\n",
        "num_classes =  ds_info.features[\"label\"].num_classes\n",
        "\n",
        "ds_train = ds_train.shuffle(shuffle_buffer, reshuffle_each_iteration=True)\n",
        "ds_train = ds_train.map(prepare_example, num_parallel_calls=autotune)\n",
        "ds_train = ds_train.batch(batch_size).prefetch(autotune)\n",
        "\n",
        "ds_test = ds_test.map(prepare_example, num_parallel_calls=autotune)\n",
        "ds_test = ds_test.batch(test_batch_size).prefetch(autotune)"
      ],
      "metadata": {
        "id": "z-9DJUAZwqlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early-Exit Model "
      ],
      "metadata": {
        "id": "AX2crGZmAHEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block( \n",
        "    input_shape, \n",
        "    num_features, \n",
        "    add_max_pool = True,\n",
        "    name=None):\n",
        "  inp = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  x_t = lq.layers.QuantConv2D(num_features, (1, 3), padding=\"same\")(inp)\n",
        "  x_t = tf.keras.layers.BatchNormalization()(x_t)\n",
        "  x_t = tf.keras.layers.Activation(\"relu\")(x_t)\n",
        "  x_f = lq.layers.QuantConv2D(num_features, (3, 1), padding=\"same\")(inp)\n",
        "  x_f = tf.keras.layers.BatchNormalization()(x_f)\n",
        "  x_f = tf.keras.layers.Activation(\"relu\")(x_f)\n",
        "\n",
        "  x = tf.keras.layers.Concatenate(axis=-1)([x_t, x_f])\n",
        "  x = lq.layers.QuantConv2D(num_features, (1, 1), padding=\"same\")(x)\n",
        "  if add_max_pool:\n",
        "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "  out = x\n",
        "  \n",
        "  return tf.keras.Model(inp, out, name=f\"conv_block_{name}\")\n",
        "\n",
        "class ExitLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.classifier = tf.keras.Sequential([\n",
        "      tf.keras.layers.GlobalMaxPooling2D(),\n",
        "      lq.layers.QuantDense(num_classes)\n",
        "    ])\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    logits = self.classifier(x)\n",
        "    return logits\n",
        "\n",
        "class EarlyExitModel(tf.keras.models.Model):\n",
        "  def __init__(self, \n",
        "    num_classes, \n",
        "    input_shape=(None, 64, 1),\n",
        "    exit_threshold=0.85):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_blocks = [\n",
        "      conv_block(input_shape, 32, name=\"conv_block1\"),\n",
        "      conv_block((None, 32, 32), 64, name=\"conv_block2\"),\n",
        "      conv_block((None, 16, 64), 128, name=\"conv_block3\"),\n",
        "      conv_block((None, 8, 128), 256, name=\"conv_block4\")\n",
        "    ]\n",
        "    self.exit_layers = [ExitLayer(num_classes) for _ in range(len(self.conv_blocks))]\n",
        "    self.exit_threshold = exit_threshold\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    if not training:\n",
        "      assert tf.shape(x)[0] == 1, \"Inference mode only supports batch size one.\"\n",
        "\n",
        "    if training:\n",
        "      outputs = []\n",
        "      for block, exit_layer in zip(self.conv_blocks, self.exit_layers):\n",
        "        x = block(x, training=training)\n",
        "        output = exit_layer(x, training=training)\n",
        "        outputs.append(output)\n",
        "      return outputs\n",
        "    else:\n",
        "      exit_id = 1\n",
        "      for conv_block, exit_layer in zip(self.conv_blocks, self.exit_layers):\n",
        "        x = conv_block(x, training=training)\n",
        "        output = exit_layer(x, training=training)\n",
        "        output_sm = tf.nn.softmax(output)\n",
        "        score = tf.reduce_max(output_sm)\n",
        "        output_pred = tf.math.argmax(output_sm, axis=-1)\n",
        "        if score >= self.exit_threshold:\n",
        "          return output_pred, exit_id\n",
        "        exit_id += 1\n",
        "      return output_pred, exit_id"
      ],
      "metadata": {
        "id": "p21_yfLsAIsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluation"
      ],
      "metadata": {
        "id": "y9yWjwYqBCda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ee_model = EarlyExitModel(num_classes=num_classes)\n",
        "ee_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "ee_model.fit(ds_train, epochs=50, verbose=2)"
      ],
      "metadata": {
        "id": "N1BYbkJeIj-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred, exits = [], [], []\n",
        "for _, (x,y) in enumerate(ds_test):\n",
        "  y_true.append(y.numpy())\n",
        "  _y, _e = ee_model(x, training=False) \n",
        "  y_pred.append(_y.numpy())\n",
        "  exits.append(_e)\n",
        "y_true = np.array(y_true).flatten()\n",
        "y_pred = np.array(y_pred).flatten()\n",
        "exits = np.array(exits)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "avg_exit = np.mean(exits)\n",
        "std_exit = np.std(exits)\n",
        "print(f\"Test set accuracy: {accuracy} | Average exit: {avg_exit} Â± {std_exit}\")"
      ],
      "metadata": {
        "id": "nwyT_iACxAgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------"
      ],
      "metadata": {
        "id": "_WiO0dhUNa48"
      }
    }
  ]
}